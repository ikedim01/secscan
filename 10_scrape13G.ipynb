{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scrape13G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape13G\n",
    "\n",
    "> Scrape holdings information from 13G SEC filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "\n",
    "from secscan import utils, dailyList, basicInfo, infoScraper\n",
    "\n",
    "default13GDir = os.path.join(utils.stockDataRoot,'scraped13G')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13G scraper class - scrape holdings information from the SEC filing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "nSharesPatStr = r'(\\d+(?:[,.]\\d\\d\\d)*)'\n",
    "nPctPatStr = r'(\\d+(?:\\.\\d*)?|\\.\\d+)'\n",
    "form13NshAndPctPats = [\n",
    "    re.compile(r'aggregate\\s+amount.{1,100}?' + nSharesPatStr\n",
    "                + r'.{1,200}?' + r'percent\\s+of\\s+class.{1,100}?' + nPctPatStr + r'\\s*%',\n",
    "                re.IGNORECASE|re.DOTALL),\n",
    "    re.compile(r'item\\s+9\\s*:.*?' + nSharesPatStr\n",
    "                + r'.*?' + r'item\\s+11\\s*:.*?' + nPctPatStr + r'\\s*%',\n",
    "                re.IGNORECASE|re.DOTALL),\n",
    "    re.compile(r'aggregate\\s+amount.{1,100}?' + nSharesPatStr\n",
    "                + r'.{1,200}?' + r'percent\\s+of class.{1,100}?(?!\\D9\\D)\\D' + nPctPatStr,\n",
    "                re.IGNORECASE|re.DOTALL),\n",
    "]\n",
    "def getSec13NshAndPctFromText(txt) :\n",
    "    \"Returns a list [(nShares, percent) ... ] parsed from form 13G or 13D.\"\n",
    "    for pat in form13NshAndPctPats :\n",
    "        res = pat.findall(txt)\n",
    "        if res :\n",
    "            break\n",
    "    return res\n",
    "\n",
    "def parse13G(accNo, formType=None) :\n",
    "    info = basicInfo.getSecFormInfo(accNo, formType=formType)\n",
    "    links = info['links']\n",
    "    if len(links) == 0 :\n",
    "        print('NO LINKS LIST!')\n",
    "        info['positions'] = []\n",
    "    else :\n",
    "        toFormat = 'text' if links[0][3].endswith('.txt') else 'souptext'\n",
    "        mainText = utils.downloadSecUrl(links[0][3], toFormat=toFormat)\n",
    "        info['positions'] = getSec13NshAndPctFromText(mainText)\n",
    "    if len(info['positions']) == 0 :\n",
    "        print('no positions found!')\n",
    "    return info\n",
    "\n",
    "class scraper13G(infoScraper.scraperBase) :\n",
    "    def __init__(self, infoDir=default13GDir, startD=None, endD=None, fSuff='m.pkl', **pickle_kwargs) :\n",
    "        super().__init__(infoDir, 'SC 13G', startD=startD, endD=endD, fSuff=fSuff, **pickle_kwargs)\n",
    "    def scrapeInfo(self, accNo, formType=None) :\n",
    "        return parse13G(accNo, formType), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 13G scraper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210703 WEEKEND 20210702 ### list index 21 filings for 20210702: 6569 * 20210701 filings for 20210701: 5573 * "
     ]
    }
   ],
   "source": [
    "dl = dailyList.dailyList(startD='empty')\n",
    "dl.updateForDays('20210701','20210704')\n",
    "assert len(dl.getFilingsList(None,'SC 13G')[0])==100,\"testing 13G scraper class (daily list count)\"\n",
    "info = parse13G('0001567619-21-013814', formType='SC 13G')\n",
    "assert (info['ciks']==['0000016099', '0001373604']\n",
    "        and info['positions']==[('1350552', '4.36'), ('1582235', '5.10')]\n",
    "    ),\"testing 13G scraper class (parsing)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# uncomment and run to regenerate all library Python files\n",
    "# from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
