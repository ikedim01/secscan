{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp infoScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infoScraper\n",
    "\n",
    "> Base class to scrape and save information from SEC filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from secscan import utils, dailyList, basicInfo\n",
    "\n",
    "defaultBaseScrapeDir = os.path.join(utils.stockDataRoot,'scrapedBase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base scraper class - just scrapes basic filing information using `basicInfo.getSecFormInfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class scraperBase(object) :\n",
    "    def __init__(self, infoDir, formClass, startD=None, endD=None, fSuff='m.pkl', **pickle_kwargs) :\n",
    "        self.infoDir = infoDir\n",
    "        self.formClass = formClass\n",
    "        self.fSuff = fSuff\n",
    "        self.pickle_kwargs = dict(pickle_kwargs)\n",
    "        self.infoMap = {}\n",
    "        self.dirtySet = set()\n",
    "        if startD=='empty' :\n",
    "            return\n",
    "        self.loadDays(startD=startD, endD=endD)\n",
    "    def loadDays(self, startD=None, endD=None) :\n",
    "        self.infoMap.update(utils.loadSplitPklFromDir(self.infoDir, startK=startD, endK=endD,\n",
    "                                                      fSuff=self.fSuff, **self.pickle_kwargs))\n",
    "    def save(self) :\n",
    "        utils.saveSplitPklToDir(self.infoMap, self.infoDir, dirtySet=self.dirtySet,\n",
    "                                fSuff=self.fSuff, **self.pickle_kwargs)\n",
    "        self.dirtySet.clear()\n",
    "    def saveDays(self, daySet) :\n",
    "        utils.saveSplitPklToDir(self.infoMap, self.infoDir, dirtySet=daySet,\n",
    "                                fSuff=self.fSuff, **self.pickle_kwargs)\n",
    "        self.dirtySet.difference_update(daySet)\n",
    "    def scrapeInfo(self, accNo, formType=None) :\n",
    "        return basicInfo.getSecFormInfo(accNo, formType), None\n",
    "    def saveXInfo(self, dStr, accNo, xInfo) :\n",
    "        utils.savePklToDir(os.path.join(self.infoDir,dStr), accNo+'-xinfo.pkl', xInfo, **self.pickle_kwargs)\n",
    "    def loadXInfo(self, dStr, accNo) :\n",
    "        return utils.loadPklFromDir(os.path.join(self.infoDir,dStr), accNo+'-xinfo.pkl', None, **self.pickle_kwargs)\n",
    "    def retryErrs(self, startD=None, endD=None, justShow=False) :\n",
    "        for dStr,dInfo in self.infoMap.items() :\n",
    "            if ((startD is not None and dStr<startD)\n",
    "                or (endD is not None and endD<=dStr)) :\n",
    "                continue\n",
    "            for accNo in dInfo :\n",
    "                if dInfo[accNo] == 'ERROR' :\n",
    "                    if justShow :\n",
    "                        print(accNo,end=' ')\n",
    "                        continue\n",
    "                    print('retrying',accNo)\n",
    "                    dInfo[accNo] = self.scrapeForAccNo(accNo)\n",
    "                    if dInfo[accNo] != 'ERROR' :\n",
    "                        self.dirtySet.add(dStr)\n",
    "    def retryErrsAndSave(self, startD=None, endD=None) :\n",
    "        self.retryErrs(startD=startD, endD=endD)\n",
    "        self.save()\n",
    "    def showErrs(self, startD=None, endD=None) :\n",
    "        self.retryErrs(startD=startD, endD=endD, justShow=True)\n",
    "    def checkDates(self, verbose=True) :\n",
    "        \"Prints info on dates present, checking for missing dates.\"\n",
    "        dailyList.checkMapDates(self.infoMap, verbose=verbose)\n",
    "    def printCounts(self, startD=None, endD=None, verbose=True) :\n",
    "        if verbose :\n",
    "            print()\n",
    "            print('Counts by day:')\n",
    "        tot = 0\n",
    "        for dStr in sorted(self.infoMap.keys()) :\n",
    "            if ((startD is not None and dStr<startD)\n",
    "                or (endD is not None and endD<=dStr)) :\n",
    "                continue\n",
    "            dCount = len(self.infoMap[dStr])\n",
    "            if verbose :\n",
    "                print(f'{dStr}: {dCount}')\n",
    "            tot += dCount\n",
    "        print('Total filings:',tot)\n",
    "    def scrapeForAccNo(self, accNo, formType=None) :\n",
    "        try :\n",
    "            info, xInfo = self.scrapeInfo(accNo, formType)\n",
    "            if xInfo is not None :\n",
    "                info['hasXInfo'] = True\n",
    "                self.saveXInfo(dStr, accNo, xInfo)\n",
    "            return info\n",
    "        except Exception as e :\n",
    "            print('*** ERROR ***',e)\n",
    "            return 'ERROR'\n",
    "    def updateForDays(self, dl, startD=None, endD=None, ciks=None, errLimitPerDay=25,\n",
    "                      verbose=True, saveAfterEachDay=False) :\n",
    "        \"\"\"\n",
    "        Update to reflect the filings for dates between startD (inclusive)\n",
    "        and endD (exclusive). If startD is None, uses the last date already\n",
    "        in self.infoMap, or the start of the current year if self.infoMap is empty.\n",
    "        If endD is None, uses today. The dl argument should be a dailyList object\n",
    "        that includes those dates.\n",
    "        Optionally restricts to a given set of CIKs.\n",
    "        \"\"\"\n",
    "        if startD is None :\n",
    "            if len(self.infoMap) == 0 :\n",
    "                startD = utils.toDateStr()[:4]+'0101' # start of current year\n",
    "            else :\n",
    "                startD = max(self.infoMap.keys())\n",
    "        for dStr in reversed(utils.dateStrsBetween(startD,endD)) :\n",
    "            if dStr not in dl.dl :\n",
    "                print('date',dStr,'not found in dailyList, aborting update!')\n",
    "                return\n",
    "            dayIsDirty = (dStr not in self.infoMap)\n",
    "            if dayIsDirty :\n",
    "                self.infoMap[dStr] = {}\n",
    "            if verbose or dayIsDirty :\n",
    "                print(f'=========={\"NEW \" if dayIsDirty else \"\"}{dStr}==========', end=' ', flush=True)\n",
    "            errCount = 0\n",
    "            dInfo = self.infoMap[dStr]\n",
    "            for cik, formType, accNo, fileDate in dl.dl[dStr] :\n",
    "                if (accNo in dInfo\n",
    "                    or (ciks is not None and cik not in ciks)\n",
    "                    or not dailyList.isInFormClass(self.formClass, formType)) :\n",
    "                    continue\n",
    "                print(f\"'{accNo}'\", end=' ', flush=True)\n",
    "                dInfo[accNo] = self.scrapeForAccNo(accNo,formType)\n",
    "                dayIsDirty = True\n",
    "                if dInfo[accNo] == 'ERROR' :\n",
    "                    errCount += 1\n",
    "                    if errCount >= errLimitPerDay :\n",
    "                        print('Error limit exceeded, aborting update!')\n",
    "                        return\n",
    "            if dayIsDirty :\n",
    "                self.dirtySet.add(dStr)\n",
    "                if saveAfterEachDay :\n",
    "                    self.save()\n",
    "    def loadAndUpdate(self, dlOrDir=dailyList.defaultDLDir,\n",
    "                      startD=None, endD=None, ciks=None, errLimitPerDay=10,\n",
    "                      verbose=True, saveAfterEachDay=False) :\n",
    "        \"\"\"\n",
    "        Loads a dailyList for the given date range (this must already have been saved),\n",
    "        and then updates the scraper for the given date range and saves it.\n",
    "        If startD is None, uses the last date already in self.infoMap, or the start of\n",
    "        the current year if self.infoMap is empty. If endD is None, uses today.\n",
    "        Optionally restricts to a given set of CIKs.\n",
    "        A scraperBase or subclass can be initialized for a date range starting from an empty directory by:\n",
    "            s = scraperBase(emptyDir, formClass, startD='empty')  # or s = subclass(startD='empty', ...)\n",
    "            s.loadAndUpdate(startD=drangeStart, endD=drangeEnd)\n",
    "            s.printCounts()\n",
    "        assuming the dailyList is already saved for that date range.\n",
    "        This will also work to extend a scraperBase or subclass to a new date range.\n",
    "        \"\"\"\n",
    "        if isinstance(dlOrDir, dailyList.dailyList) :\n",
    "            dl = dlOrDir\n",
    "        else :\n",
    "            dl = dailyList.dailyList(dlDir=dlOrDir, startD=startD, endD=endD)\n",
    "        self.loadDays(startD=startD, endD=endD)\n",
    "        self.updateForDays(dl, startD=startD, endD=endD, ciks=ciks, errLimitPerDay=errLimitPerDay,\n",
    "                           verbose=verbose, saveAfterEachDay=saveAfterEachDay)\n",
    "        self.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test base scraper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210702 ### list index 6 filings for 20210702: 6569 * 20210702 "
     ]
    }
   ],
   "source": [
    "dl = dailyList.dailyList(startD='empty')\n",
    "dl.updateForDays('20210702','20210703')\n",
    "assert len(dl.getFilingsList(None,'10-K')[0])==6,\"testing base scraper class (daily list count)\"\n",
    "\n",
    "b = scraperBase(defaultBaseScrapeDir,'10-K')\n",
    "b.updateForDays(dl,'20210702','20210703')\n",
    "assert len(b.infoMap['20210702'])==6,\"testing base scraper class (info count)\"\n",
    "\n",
    "links = b.infoMap['20210702']['0001640334-21-001482']['links']\n",
    "assert (len(links)==9\n",
    "        and links[0]==('ptco_10k.htm','FORM 10-K','10-K',\n",
    "                       '/Archives/edgar/data/1609258/000164033421001482/ptco_10k.htm')\n",
    "        and links[-1]== ('ptco-20210331_def.xml','XBRL TAXONOMY EXTENSION DEFINITION LINKBASE','EX-101.DEF',\n",
    "                         '/Archives/edgar/data/1609258/000164033421001482/ptco-20210331_def.xml')\n",
    "       ), \"testing base scraper class (scraped info)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# uncomment and run to regenerate all library Python files\n",
    "# from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
