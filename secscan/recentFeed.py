# AUTOGENERATED! DO NOT EDIT! File to edit: 01_recentFeed.ipynb (unless otherwise specified).

__all__ = ['secMostRecentListUrl', 'printXmlParseWarning', 'getRecentChunk', 'titlePat', 'filedPat',
           'curEasternTimeStampAndDate', 'initRecentFeedS3', 'updateRecentFeedS3', 'getRecentFromS3',
           'getRecentFromS3Public']

# Cell

import re
import xml.etree.cElementTree as cElTree

from secscan import utils

# Cell

def secMostRecentListUrl(count=100) :
    "Returns the URL for the SEC's atom-format feed of most recent filings."
    return ('/cgi-bin/browse-edgar?'
            +('' if count is None else f'count={count}&')
            +'action=getcurrent&output=atom')

def printXmlParseWarning(msg,el) :
    print('***',msg,'***')
    print(cElTree.tostring(el))
    print('************************')

titlePat = re.compile(
        r"\s*(.+?)\s+-" # formType, ignoring surrounding whitespace
        + r"\s+(.+?)\s*" # cikName, ignoring surrounding whitespace
        + r"\((\d{10})\)") # cik
filedPat = re.compile(
        r"filed\D+?\s(\d\d\d\d[-/]?\d\d[-/]?\d\d)\s.*"
        + r"accno\D+?\s("+utils.accessNoPatStr+r")\s",
        re.IGNORECASE)
def getRecentChunk(count=100) :
    """
    Parses the SEC's atom-format feed of most recent filings and returns a list of tuples:
        [(fileDate, cikName, accNo, formType, cik),
         ... ]
    with the most recent filings first
    """
    mrListXml = utils.downloadSecUrl(secMostRecentListUrl(count=count), toFormat='xml')
    res = []
    for listEntry in mrListXml :
        if not listEntry.tag.lower().endswith("entry") :
            continue
        cik = formType = accNo = fDate = cikName = None
        for entryItem in listEntry :
            itemTag = entryItem.tag.lower()
            if itemTag.endswith('title') :
                # print('"'+entryItem.text.strip()+'"')
                m = titlePat.match(entryItem.text)
                if m is None :
                    printXmlParseWarning('unable to parse title element',listEntry)
                    continue
                formType,cikName,cik = m.groups()
                cik = cik.lstrip('0')
                # print(repr(formType),repr(cikName),repr(cik))
            elif itemTag.endswith('summary') :
                # print('"'+entryItem.text.strip()+'"')
                m = filedPat.search(entryItem.text)
                if m is None :
                    printXmlParseWarning('unable to parse summary element',listEntry)
                    continue
                fDate,accNo = m.groups()
                # print(repr(fDate),repr(accNo))
        fTup = (fDate, cikName, accNo, formType, cik)
        if all(fTup) :
            res.append(fTup)
    return res

# Cell

def curEasternTimeStampAndDate() :
    nowET = utils.curEasternUSTime()
    ts = nowET.isoformat().replace('T',' ')
    return nowET, ts[:19], ts[:10]

def initRecentFeedS3(bucket, prevDay=None) :
    _, curTS, today = curEasternTimeStampAndDate()
    utils.pickSaveToS3(bucket, 'today-feed.pkl',
                       {'updated':curTS, 'filings':set(), 'curDay':today, 'prevDay':None},
                       use_gzip=True, make_public=True, protocol=2)

def updateRecentFeedS3(bucket, skipOffHours=True) :
    nowET, curTS, today = curEasternTimeStampAndDate()
    print('updating at', curTS, end='; ')
    if skipOffHours and (utils.isWeekend(nowET)
                         #or nowET.hour<6 or nowET.hour>22
                         #or (nowET.hour==22 and nowET.minute>10)
                        ) :
        print('SEC off hours, skipping update')
        return
    l = getRecentChunk()
    curFeed = utils.pickLoadFromS3(bucket, 'today-feed.pkl', use_gzip=True)
    print('last update', curFeed['updated'])
    if today != curFeed['curDay'] :
        print('starting new day; last day found was',curFeed['curDay'])
        utils.pickSaveToS3(bucket, curFeed['curDay']+'-feed.pkl', curFeed,
                           use_gzip=True, make_public=True, protocol=2)
        prevFilings, prevDay = curFeed['filings'], curFeed['curDay']
        curFeed = {'filings':set(), 'curDay':today, 'prevDay':prevDay}
    elif curFeed['prevDay'] is not None :
        print('continuing current day; most recent previous day was',curFeed['prevDay'])
        prevFeed = utils.pickLoadFromS3(bucket, curFeed['prevDay']+'-feed.pkl', use_gzip=True)
        prevFilings, prevDay = prevFeed['filings'], prevFeed['curDay']
    else :
        print('continuing current day; no previous day found')
        prevFilings, prevDay = set(), None
    prevDayCount = newFTodayCount = newFOtherDayCount = 0
    for tup in l :
        if tup in curFeed['filings'] :
            continue
        if tup in prevFilings :
            prevDayCount += 1
            continue
        curFeed['filings'].add(tup)
        fDate = tup[0]
        if fDate == today :
            newFTodayCount += 1
        else :
            newFOtherDayCount += 1
            if fDate < today :
                print('*** old filing date',tup)
            else :
                print('*** unexpected future filing date',tup)
    print(len(l),'filings,',
          prevDayCount,'from prev day,',newFTodayCount,'new fToday,',newFOtherDayCount,'new fOther,',
          'total now',len(curFeed['filings']))
    curFeed['updated'] = curTS
    utils.pickSaveToS3(bucket, 'today-feed.pkl', curFeed,
                       use_gzip=True, make_public=True, protocol=2)
    print('--- update complete at',curEasternTimeStampAndDate()[1])

def getRecentFromS3(bucket, key='today') :
    return utils.pickLoadFromS3(bucket, key+'-feed.pkl', use_gzip=True)

def getRecentFromS3Public(bucket, key='today') :
    return utils.pickLoadFromS3Public(bucket, key+'-feed.pkl', use_gzip=True)