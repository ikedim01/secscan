# AUTOGENERATED! DO NOT EDIT! File to edit: 10_scrape13G.ipynb (unless otherwise specified).

__all__ = ['default13GDir', 'parse13G', 'scraper13G']

# Cell

import collections
import itertools
import os
import re

from secscan import utils, dailyList, basicInfo, infoScraper

default13GDir = os.path.join(utils.stockDataRoot,'scraped13G')

# Cell

def parse13G(accNo, formType=None) :
    info = basicInfo.getSecFormInfo(accNo, formType=formType, get99=True, textLimit=textLimit)
    links = info['links']
    if len(links) == 0 :
        utils.printErrInfoOrAccessNo('NO LINKS LIST in',accNo)
        return info
    toFormat = 'text' if links[0][3].endswith('.txt') else 'souptext'
    mainText = utils.downloadSecUrl(links[0][3], toFormat=toFormat)
    return info, mainText

class scraper13G(infoScraper.scraperBase) :
    def __init__(self, infoDir=default13GDir, startD=None, endD=None, fSuff='m.pkl', **pickle_kwargs) :
        super().__init__(infoDir, 'SC 13G', startD=startD, endD=endD, fSuff=fSuff, **pickle_kwargs)
    def scrapeInfo(self, accNo, formType=None) :
        return parse13G(accNo, formType), None